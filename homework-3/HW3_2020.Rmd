---
title: ANLYTC3 Homework 3
author: Julian Terry S. Bass
output: html_document
---

#### Instructions 

The due date is 09:30AM on Feb 23 (Tue). Submit BOTH the Knit HTML file and the Rmd file.

Code should be clearly commented. Plots should be presentable and properly labeled/titled. Mitigate overplotting whenever possible. 

You may hard code whatever written answers are required.

#### Preliminaries

Here are some libraries that you may need.
```{r}
library(ggplot2)
library(plyr)
library(reshape2)
library(knitr)
library(binom) 
```

We will use the data files `county_data.csv` and `county_ggplot.rda`, which were part of the lecture materials. We'll assume they are in the same directory as this .Rmd file.

```{r}
county.data = read.csv(file = 'county_data.csv', header=TRUE)
load('county_ggplot.rda')
```


#### Questions 

**Problem 1:** 

Make a scatterplot of the per capita violent crime rates for each county as function of the population. Remember to mitigate overplotting. Does this plot resemble those for deaths, births, or infant deaths that we saw in the notes? If not, what is the biggest difference?

Note: you may want to use a log transform on the x-axis to see the data more clearly.

```{r fig.width=6, fig.height=4, dpi=80, fig.align='center'}
# fill in

# Scatterplot function that takes in a per capita column value, a function (in this case population) and a plot title
per.capita.scatterplot = function(x, n, main.title) {
  CI.data = with(county.data, binom.confint(x = x, n = n, methods = 'exact'))

  county.augment = with(CI.data, 
      mutate(county.data, 
      lower.CI = lower, upper.CI = upper, per.capita = mean))

  kable( arrange(county.augment, desc(x))[, c("name", "population", "violent.crimes", "per.capita", "lower.CI")], digits=4)

  ggplot(county.augment, aes(x=n, y=per.capita)) + geom_point() + geom_smooth() + scale_x_log10() + labs(x = "Population", y = "Per Capita", title = main.title)
  
}

# Violent Crimes by Population
per.capita.scatterplot(county.data$violent.crimes, county.data$population, "Violent Crimes by Population")

# Births by Population
per.capita.scatterplot(county.data$births, county.data$population, "Births by Population")

# Deaths by Population
per.capita.scatterplot(county.data$deaths, county.data$population, "Deaths by Population")

# Infant Deaths by Population
per.capita.scatterplot(county.data$infant.deaths, county.data$population, "Infant Deaths by Population")


```

ANS: I'll admit that the results from above don't seem to logically fit each other. Despite seeing an increase in violent crimes with greater populations, we see a decrease in deaths. You'd think more deaths would come up in this case.

Birth data seems a bit more promising in that with higher populations with more births, we see the rate of infant deaths stagnate and level out with a general decrease. This means that while more babies are born in higher populations, infant death rates go down.

**Problem 2:**

Create a function called `find.std.residual()`, which lets you specify a column in the `county.data` data frame, and then uses the p-value approach to identify counties where people may potentially be at higher risk. 

Specifically, your function should take the following inputs

* the `county.data` data set
* `variable`: the name of the column that you want to inspect
* `null.prob`: this variable determines the null hypothesis that you will compare all counties to. Specifically, the null is that all counties are `binomial(population, null.prob)` random variables.

Your function should return a data frame with all of the columns in `county.data`, plus the following additional columns

* `expected.null`: the expected number of outcomes under the null hypothesis for each county
* `variance.null`: the variance under the null hypothesis for each county
* `residual`: the difference between the observed count and that predicted under the null
* `std.residual`: the standardized residual

The rows of the data frame should be sorted in decreasing order by `std.residual`

Be sure to comment your function thoroughly so that it is easy to understand later.

```{r fig.width=8, fig.height=6, dpi=100, fig.align='center'}
# fill in

find.std.residual = function(data, variable, null.prob) {
  
  data = mutate(data, expected.null = variable * null.prob,
                std.dev.null = sqrt(expected.null * (1 - null.prob)),
                residual = variable - expected.null,
                std.residual = residual / std.dev.null)
  
  # Decreasing data order by std.residual
  return(data.frame(arrange(data, desc(std.residual))))
}


```

**Problem 3:**

Use your function from problem 2 to create a table showing the top fifteen counties for `violent.crimes`, under a null hypothesis that each county has a crime per capita rate of 1%.

```{r fig.width=8, fig.height=6, dpi=100, fig.align='center'}
# fill in

# Initial function call
# Retrieving only the top 15 records by violent crime rates
top.15.crimes = find.std.residual(county.data, county.data$violent.crimes, 0.01)[1:15, ]

top.15.crimes

```

**Problem 4:**

Create a function `map.std.residual()` which takes the output of your function `find.std.residual()`, and creates a choropleth plot.

The inputs to the function should be

* `county.gg`: the data frame needed to draw the map
* `county.data`: the output from `find.std.residual()`

The output should be a map that can be printed, to which you can customize later by adding calls like `scale_fill_gradient2()` and `labs()`

Use this function to create a choropleth plot of the results from problem 3.

```{r fig.width=10, fig.height=6, dpi=100, fig.align='center'}
# fill in


# ----------Code taken from Exercise 6 solution + Help from Zinedine----------

# Creation of the map.std.residual function with 2 parameters which are county.gg, county.data
map.std.residual = function(county.gg, county.data) {
  
  # Getting the US.per.capita by dividing the sum of violent.crimes to population
  US.per.capita = sum(county.data$violent.crimes) / sum(county.data$population)
  
  # Merging the 2 data frames by the columns fips and STCOU
  county.merge = merge(x = county.gg, y = county.data, by.x = "fips", by.y = "STCOU")
  
  # Arranging the data frame county.merge according to order
  county.merge = arrange(county.merge, order)
  
  # Starting of data map_data
  county_list = map_data("county")
  
  # ggplot automatically returns value of the map.std.residual function that creates the map
ggplot() + geom_map(data = county_list, map = county_list, mapping = aes(x = long, y = lat, map_id = region), col = "white", fill = "grey") + geom_polygon(data = county.merge, mapping = aes(x = long, y = lat, group = group, fill = std.residual)) + coord_map() + labs(x = "Longitude", y = "Latitude", fill = "Standard Residual", group = "Counties", title = "Top 15 Counties \n with High Standard Residual") + scale_fill_gradientn(limits = c(0,4000), colours=c("grey", "violet", "blue"), breaks=c(0, 1000 , 2000, 3000, 4000))

}

# Running the function with the appropriate data sets and adding some styles to the created map
map.std.residual(county.gg, top.15.crimes) 

```

**Problem 5:**

Using the functions you wrote, make a new choropleth plot and table, this time using the US violent crime per capita rate as your null hypothesis. 

```{r fig.width=10, fig.height=6, dpi=100, fig.align='center'}
# fill in

# Getting the Violent Crime per Capita Rate
crime.rate.per.capita = sum(county.data$violent.crimes) / sum(county.data$population)

# Passing the crime rate as the null hypothesis
map.std.residual(county.gg, find.std.residual(county.data, county.data$violent.crimes, crime.rate.per.capita)[1:15,])

```

Comment on why the map and table from problem 5 are different from those of problems 4 and 3.

It looks like the shaded regions are the same but with higher standard residual concentrations for the results of problem 5. When you look at the structure of the find.std.residual() function, you see a list of calculations leading up to the standard residual which also depend on the null hypothesis passed in. Since we are passing in a different calculated hypothesis in problem 5, it affects that chain of calculations and adjusts the intensity of the colored results on the map.

Interestingly enough, it still seems to maintain the same top 15 counties but with varied standard residual results.


